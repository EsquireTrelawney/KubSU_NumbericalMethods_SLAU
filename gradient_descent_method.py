import numpy as np

# Метод наискорейшего спуска для решения СЛАУ
# Принимает matrix - матрицу коэфф, column_vector - вектор свободных коэфф, x0 - начальное приближение решения
# ... epsilon - критерий остановки, допустимая разница между текущим и предыдущим решением
# ... max_iterations - максимальное кол-во итераций
def gradient_descent_solve(matrix, column_vector, epsilon=1e-6, max_iterations=1000):
    iterations = 0
    x0 = np.zeros_like(column_vector)  # создаём вектор с начальным приближением решения. Нуль-вектор с таким же кол-вом
    # ... элементов, что и у вектора свободных коэфф
    x = x0
    r = column_vector - np.dot(matrix, x)  # Вычисление вектора невязки для метода (отличие полученных столбцов свободных
                                           # коэфф от требуемых. Каждое решение ведь не является сразу точным и при
                                           # умножении на матрицу коэфф даёт немного другие свободные коэфф)

    # Выполняем итерации, пока норма вектора невязки
    # не станет меньше требуемой погрешности И кол-во итераций меньше максимального кол-ва итераций. По сути,
    # пока что-то одно из этого не станет ложью
    while np.linalg.norm(r) > epsilon and iterations < max_iterations:
        # альфа - шаг градиентного спуска.
        alpha = np.dot(r, r) / np.dot(np.dot(matrix, r), r)
        # обновляем полученное решение
        x = x + alpha * r
        # ... и вектор невязки тоже
        r = column_vector - np.dot(matrix, x)
        iterations += 1

    # возвращаем вектор решений и кол-во итераций
    return x, iterations